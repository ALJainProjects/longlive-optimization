# LongLive Optimized Inference Configuration
# SPDX-License-Identifier: Apache-2.0
#
# This config applies recommended optimizations for low-latency inference.
# Target: <40ms per frame (~25+ FPS)
#
# Optimizations applied:
# - Reduced attention window (12 -> 9 frames)
# - Frame sink preserved for consistency
# - FP8 quantization enabled
# - torch.compile enabled

denoising_step_list:
- 1000
- 750
- 500
- 250
warp_denoising_step: true
num_frame_per_block: 3
model_name: Wan2.1-T2V-1.3B

model_kwargs:
  local_attn_size: 9      # Reduced from 12 for ~10% speedup
  timestep_shift: 5.0
  sink_size: 3            # Keep frame sink for consistency

# inference
data_path: longlive_models/prompts/vidprom_filtered_extended.txt
output_folder: videos/optimized
inference_iter: -1
num_output_frames: 120
use_ema: false
seed: 0
num_samples: 1
save_with_index: true
global_sink: true
context_noise: 0

generator_ckpt: longlive_models/models/longlive_base.pt
lora_ckpt: longlive_models/models/lora.pt

adapter:
  type: "lora"
  rank: 256
  alpha: 256
  dropout: 0.0
  dtype: "bfloat16"
  verbose: false

# Optimization flags (used by optimized inference script)
optimization:
  use_torch_compile: true
  compile_mode: "reduce-overhead"  # or "max-autotune" for H100
  use_fp8: true
  use_cuda_graph: false  # Experimental, may cause issues with dynamic shapes
  profile: true
